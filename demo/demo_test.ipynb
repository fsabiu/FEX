{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/shared/datasets_for_training/YOLO_military_Trucks/runs/detect/train2/weights/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/FEX/demo/demo_test.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     display_predictions(images, predictions, ground_truths)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Replace with actual paths\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m test_yolo(\u001b[39m'\u001b[39;49m\u001b[39m~/shared/datasets_for_training/YOLO_military_Trucks/runs/detect/train2/weights/best.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39m~/shared/datasets_for_training/YOLO_military_Truck/test/images\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39m~/shared/datasets_for_training/YOLO_military_Truck/test/labels\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/ubuntu/FEX/demo/demo_test.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_yolo\u001b[39m(model_path, images_path, labels_path, img_size\u001b[39m=\u001b[39m\u001b[39m640\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Load the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     model \u001b[39m=\u001b[39m YOLO(model_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# Load test images and annotations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B138.3.249.228/home/ubuntu/FEX/demo/demo_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     images \u001b[39m=\u001b[39m []  \u001b[39m# List to store test images\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/ultralytics/models/yolo/model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m new_instance\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[39m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model\u001b[39m=\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49mtask, verbose\u001b[39m=\u001b[39;49mverbose)\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/ultralytics/engine/model.py:152\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(model, task\u001b[39m=\u001b[39mtask, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load(model, task\u001b[39m=\u001b[39;49mtask)\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/ultralytics/engine/model.py:241\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    238\u001b[0m weights \u001b[39m=\u001b[39m checks\u001b[39m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[39m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m Path(weights)\u001b[39m.\u001b[39msuffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39m=\u001b[39m attempt_load_one_weight(weights)\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs[\u001b[39m\"\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverrides \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset_ckpt_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/ultralytics/nn/tasks.py:806\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattempt_load_one_weight\u001b[39m(weight, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fuse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    805\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m     ckpt, weight \u001b[39m=\u001b[39m torch_safe_load(weight)  \u001b[39m# load ckpt\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mDEFAULT_CFG_DICT, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(ckpt\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtrain_args\u001b[39m\u001b[39m\"\u001b[39m, {}))}  \u001b[39m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     model \u001b[39m=\u001b[39m (ckpt\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mema\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m ckpt[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()  \u001b[39m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/ultralytics/nn/tasks.py:732\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     \u001b[39mwith\u001b[39;00m temporary_modules(\n\u001b[1;32m    726\u001b[0m         {\n\u001b[1;32m    727\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39multralytics.yolo.utils\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39multralytics.utils\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m         }\n\u001b[1;32m    731\u001b[0m     ):  \u001b[39m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m         ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    734\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    445\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO-W/lib/python3.10/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/shared/datasets_for_training/YOLO_military_Trucks/runs/detect/train2/weights/best.pt'"
     ]
    }
   ],
   "source": [
    "def display_predictions(images, predictions, ground_truths):\n",
    "    for img, pred, gt in zip(images, predictions, ground_truths):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for plotting\n",
    "        \n",
    "        # Draw ground truth boxes\n",
    "        for gt_box in gt:\n",
    "            x1, y1, x2, y2, cls = gt_box\n",
    "            img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            img = cv2.putText(img, f'{cls}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw predicted boxes\n",
    "        for pred_box in pred:\n",
    "            x1, y1, x2, y2, conf, cls = pred_box\n",
    "            img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "            img = cv2.putText(img, f'{cls} {conf:.2f}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def test_yolo(model_path, images_path, labels_path, img_size=640):\n",
    "    # Load the model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Load test images and annotations\n",
    "    images = []  # List to store test images\n",
    "    ground_truths = []  # List to store ground truth boxes\n",
    "\n",
    "    for img_file in os.listdir(images_path):\n",
    "        if img_file.endswith('.jpg'):\n",
    "            img_path = os.path.join(images_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            images.append(img)\n",
    "\n",
    "            # Load ground truth annotations (assuming they are in YOLO format)\n",
    "            annot_file = img_file.replace('.jpg', '.txt')\n",
    "            annot_path = os.path.join(labels_path, annot_file)\n",
    "            gt_boxes = []\n",
    "            with open(annot_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    cls = int(parts[0])\n",
    "                    x_center, y_center, width, height = map(float, parts[1:])\n",
    "                    x1 = (x_center - width / 2) * img.shape[1]\n",
    "                    y1 = (y_center - height / 2) * img.shape[0]\n",
    "                    x2 = (x_center + width / 2) * img.shape[1]\n",
    "                    y2 = (y_center + height / 2) * img.shape[0]\n",
    "                    gt_boxes.append((x1, y1, x2, y2, cls))\n",
    "            ground_truths.append(gt_boxes)\n",
    "\n",
    "    # Run predictions\n",
    "    predictions = []\n",
    "    for img in images:\n",
    "        results = model.predict(img, imgsz=img_size)\n",
    "        pred_boxes = []\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy\n",
    "                conf = box.conf\n",
    "                cls = box.cls\n",
    "                pred_boxes.append((x1.item(), y1.item(), x2.item(), y2.item(), conf.item(), int(cls.item())))\n",
    "        predictions.append(pred_boxes)\n",
    "\n",
    "    # Display predictions and ground truths\n",
    "    display_predictions(images, predictions, ground_truths)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Replace with actual paths\n",
    "test_yolo('~/shared/datasets_for_training/YOLO_military_Trucks/runs/detect/train2/weights/best.pt',\n",
    "          '~/shared/datasets_for_training/YOLO_military_Truck/test/images',\n",
    "          '~/shared/datasets_for_training/YOLO_military_Truck/test/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: ~/shared: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cd '~/shared'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO-W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
